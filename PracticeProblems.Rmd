---
title: "Practice Problems in R"
author: "Peter Sun"
date: "September 30-31, 2021"
output:
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: 4
header-includes:
- \usepackage{float}
- \usepackage{pdflscape}
- \usepackage{threeparttable}
- \usepackage{threeparttablex}
- \usepackage{pgfornament}
- \usepackage{mathtools}
- \usepackage{amsmath,amsthm,amssymb}
- \newcommand{\blandscape}{\begin{landscape}}
- \newcommand{\elandscape}{\end{landscape}}
---

\newpage
# Practice 1: Generalized Boosted Regression and Propensity Score Weighting

## Problem 1: Generalized Boosted Regression

### Load Packages

```{r message=F, warning=F, error=F}
library(tidyverse)
library(haven)
library(sjlabelled)
library(lmtest)
library(gbm)
library(modelr)
library(broom)
library(sandwich)
library(cobalt)
library(WeightIt)
library(Matching)
library(kableExtra)
select <- dplyr::select
```

### Load and Randomly Shuffle Data

```{r}
d <- read_dta("data/ldw_exper.dta") %>%
  haven::zap_formats() %>%
  sjlabelled::remove_all_labels() %>%
  as_tibble()
set.seed(1000)
d2 <- d %>%
  add_column(runif = runif(nrow(.))) %>%
  arrange(runif)
```

### Generate Propensity Scores

```{r message=F}
set.seed(1000)
m1 <- gbm::gbm(formula = t ~ age + educ + black + hisp + married + re74 +
                         re75 + u74 + u75,
               data = d2,
               distribution = "bernoulli",
               n.trees = 1000,
               train.fraction = 0.8,
               interaction.depth = 4,
               shrinkage = 0.0005)

# Estimate Propensity Scores and Obtain Summary Statistics
d3 <- d2 %>%
  modelr::add_predictions(m1, var = "psb", type = "response")
summary(d3$psb)
```

\newpage
### Histograms of Propensity Scores

```{r fig.width=6, fig.height=3.5, fig.align="center"}
d3 %>%
  mutate(t = factor(t, labels = c("Control", "Treatment"))) %>%
  ggplot(aes(x = psb, color = t)) + 
  theme_classic() +
  geom_histogram(aes(fill = t), alpha = 0.6, bins = 13) +
  geom_density(size = 1) + 
  labs(x = "Predicted Probability", y = "Density",
       title = "Histograms of Estimated Propensity Scores") +
  theme(legend.position = "none") +
  facet_wrap(~ t)
```

\newpage
### Boxplots of Propensity Scores

```{r fig.width=6, fig.height=3.5, fig.align="center"}
d3 %>%
  mutate(t = factor(t, labels = c("Control", "Treatment"))) %>%
  ggplot(aes(x = t, y = psb, color = t, fill = t)) + 
  theme_classic() +
  geom_boxplot(alpha = 0.7) +
  labs(x = "Treatment Condition",
       y = "Predicted Probability",
       title = "Boxplots of Estimated Propensity Scores") +
  theme(legend.position = "none")
```

\newpage
## Problem 2: Propensity Score Weighting

### Estimate ATE and ATT Weights

```{r message=FALSE, warning=FALSE}
d4 <- d3 %>%
  mutate(ate_w = ifelse(t == 0, 1/(1-psb), 1/psb),
         att_w = ifelse(t == 0, psb/(1-psb), 1))

# Import Stata-generated weights to replicate Stata results
stata_weights <- read_dta("data/ldw1.dta") %>%
  haven::zap_formats() %>%
  sjlabelled::remove_all_labels() %>%
  as_tibble() %>%
  mutate(stata_ate_w = ifelse(t == 0, 1/(1-psb), 1/psb),
         stata_att_w = ifelse(t == 0, psb/(1-psb), 1)) %>%
  select(id, stata_ate_w, stata_att_w)
d5 <- d4 %>%
  arrange(id) %>%
  left_join(stata_weights, by = "id")
```

### Outcome Analysis with ATE and ATT Weights

```{r message=FALSE, warning=FALSE}
# Define outcome formula
f = as.formula(re78 ~ t + age + educ + black + hisp + married + re74 + re75 + 
           u74 + u75)

# Weighted OLS with R-Generated Propensity Scores
m2 <- lm(f, data = d5, weights = ate_w)
tidy(lmtest::coeftest(m2, vcov. = vcovHC(m2, "HC1"))) %>% filter(term == "t") # ATE
m3 <- lm(f, data = d5, weights = att_w) 
tidy(lmtest::coeftest(m3, vcov. = vcovHC(m3, "HC1"))) %>% filter(term == "t") # ATT

# Weighted OLS with Stata-Generated Propensity Scores (Identical Results)
m2.stata <- lm(f, data = d5, weights = stata_ate_w) 
tidy(lmtest::coeftest(m2.stata, vcov. = vcovHC(m2.stata, "HC1"))) %>% 
  filter(term == "t") # ATE
m3.stata <- lm(f, data = d5, weights = stata_att_w) 
tidy(lmtest::coeftest(m3.stata, vcov. = vcovHC(m3.stata, "HC1"))) %>% 
  filter(term == "t") # ATT
```

\newpage
### Check Imbalance

See the Appendix for the custom function `robustse()` that is used to replicate the robust standard errors in Stata.

```{r include=F}
# Replicate Stata robust standard errors
robustse <- function(x, coef = c("logit", "odd.ratio", "probs")) {
  suppressMessages(suppressWarnings(library(lmtest)))
  suppressMessages(suppressWarnings(library(sandwich)))

  sandwich1 <- function(object, ...) {
    sandwich(object) *
      nobs(object) / (nobs(object) - 1)
  }
  # Function calculates SE's
  mod1 <- coeftest(x, vcov = sandwich1)
  # apply the function over the variance-covariance matrix

  if (coef == "logit") {
    return(mod1) # return logit with robust SE's
  } else if (coef == "odd.ratio") {
    mod1[, 1] <- exp(mod1[, 1]) # return odd ratios with robust SE's
    mod1[, 2] <- mod1[, 1] * mod1[, 2]
    return(mod1)
  } else {
    mod1[, 1] <- (mod1[, 1] / 4) # return probabilites with robust SE's
    mod1[, 2] <- mod1[, 2] / 4
    return(mod1)
  }
}
```

```{r}
# Function to Check Imbalance
check_bal <- function(var, weight, type) {
  if(type == "categorical") {
    m <- glm(as.formula(paste0(var, "~t")),
      family = quasibinomial,
      data = d5,
      weights = weight
    )
    m %>%
      tidy() %>%
      mutate(odds.ratio = exp(estimate), variable = var) %>%
      mutate(or.se = robustse(m, coef = "odd.ratio")[,2]) %>%
      mutate(statistic = robustse(m, coef = "odd.ratio")[,3]) %>%
      mutate(p.value = robustse(m, coef = "odd.ratio")[,4]) %>%
      select(variable, term, odds.ratio, or.se, statistic, p.value)
  } else if(type == "continuous") {
    m <- lm(as.formula(paste0(var, "~t")),
            data = d5,
            weights = weight)
    lmtest::coeftest(m, vcov. = vcovHC(m, "HC1")) %>%
      tidy() %>%
      add_column(var, .before = "term")
  }
}
format_bal <- function(df) {
  df %>%
    filter(term != "(Intercept)") %>%
    kbl(booktabs = T, digits = 7) %>%
    kable_styling(position = "center") %>%
    kable_styling(latex_options = c("striped", "HOLD_position"))
}
```

\newpage

```{r}
# Categorical Variables
cat_vars <- c("black", "hisp", "married", "u74", "u75")
format_bal(map_dfr(cat_vars, check_bal, d5$stata_ate_w, "categorical"))
format_bal(map_dfr(cat_vars, check_bal, d5$stata_att_w, "categorical"))

# Continuous Variables
cont_vars <- c("age", "educ", "re74", "re75")
format_bal(map_dfr(cont_vars, check_bal, d5$stata_ate_w, "continuous"))
format_bal(map_dfr(cont_vars, check_bal, d5$stata_att_w, "continuous"))
```

Similar results can be obtained using the R-generated propensity score weights:

```{r eval=F}
# With R-generated weights
format_bal(map_dfr(cat_vars, check_bal, d5$ate_w, "categorical"))
format_bal(map_dfr(cat_vars, check_bal, d5$att_w, "categorical"))
format_bal(map_dfr(cont_vars, check_bal, d5$ate_w, "continuous"))
format_bal(map_dfr(cont_vars, check_bal, d5$att_w, "continuous"))
```

\newpage
### Alternative Solution with the WeightIt Package

Use GBM to estimate ATE and ATT:

```{r}
set.seed(1000)
w1.out <- WeightIt::weightit(
  formula = t ~ age + educ + black + hisp + married + re74 +
                 re75 + u74 + u75,
  data = d2,
  method = "gbm",
  distribution = "bernoulli",
  stop.method = "es.mean",
  n.trees = 1000,
  nTrain = 0.8 * nrow(d2),
  interaction.depth = 4,
  shrinkage = 0.0005,
  estimand = "ATE")

set.seed(1000)
w2.out <- WeightIt::weightit(
  formula = t ~ age + educ + black + hisp + married + re74 +
                 re75 + u74 + u75,
  data = d2,
  method = "gbm",
  distribution = "bernoulli",
  stop.method = "es.mean",
  n.trees = 1000,
  nTrain = 0.8 * nrow(d2),
  interaction.depth = 4,
  shrinkage = 0.0005,
  estimand = "ATT")
```

Assess balance with the cobalt package:

```{r fig.width=6, fig.height=3.5, fig.align="center"}
cobalt::love.plot(w1.out, thresholds = c(m = .1), binary = "std", abs = T) +
  labs(title = "Covariate Balance (ATE)")
cobalt::love.plot(w2.out, thresholds = c(m = .1), binary = "std", abs = T) +
  labs(title = "Covariate Balance (ATT)")
```

For the outcome analysis, the ATE and ATT weights can be obtained with `w1.out$weights` (ATE) and `w2.out$weights` (ATT):

```{r}
m2.weightit <- lm(f, data = d2, weights = w1.out$weights) 
tidy(lmtest::coeftest(m2.weightit, vcov. = vcovHC(m2.weightit, "HC1"))) %>%
  filter(term == "t")
m3.weightit <- lm(f, data = d2, weights = w2.out$weightit) 
tidy(lmtest::coeftest(m3.weightit, vcov. = vcovHC(m3.weightit, "HC1"))) %>%
  filter(term == "t")
```

\newpage

# Practice 2: Matching Estimators

## Load Data

```{r}
p2.d <- read_dta("data/prac2.dta") %>%
  haven::zap_formats() %>%
  sjlabelled::remove_all_labels() %>%
  as_tibble()
```

## Breusch-Pagan Test for Heteroskedasticity

The homoscedasticity assumption is not valid (e.g., p-value of the test for `age97` is < .05), indicating that the conditional variance of the outcome variable was not constant across levels of child's age, therefore a robust estimation of variance is warranted.

```{r message=FALSE, warning=FALSE}
p2.m0 <- lm(lwss97 ~ kuse + male + black + age97 + pcged97 + mratio96 + pcg_adc, 
            data = p2.d)
get_bptest <- function(data, lm.model, var) {
  b <- lmtest::bptest(lm.model, as.formula(paste0("~", var)), 
                      data = data, studentize = F)
  return(tibble(variable = var, statistic = b$statistic, 
                df = b$parameter, p.value = b$p.value))
}
map_dfr(c("kuse", "male", "black", "age97", "pcged97", "mratio96", "pcg_adc"), 
        get_bptest, data = p2.d, lm.model = p2.m0) %>%
  kbl(booktabs = T, digits = 2, linesep = "",
      caption = "Results of Breusch-Pagan Tests for Heteroskedasticity") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = c("striped", "HOLD_position"))
```

\newpage
## Matching Estimators

### Define Outcome (Y), Treatment Index (Tr), and Variables to Match On (X)

```{r}
Y <- p2.d$lwss97
Tr <- p2.d$kuse
X <- select(p2.d, male, black, age97, pcged97, mratio96, pcg_adc)
```

### Define Function for Matching

```{r}
get_match <- function(estimand, sample) {
  m <- Matching::Match(Y = Y, Tr = Tr, X = X, M = 4, BiasAdjust = T, Var.calc = 4,
              estimand = estimand, sample = sample)
  return(list(
    est = m$est[,1],
    se = m$se,
    t.stat = m$est[,1]/m$se,
    p = (1 - pnorm(abs(m$est[,1]/m$se))) * 2
  ))
}
```

### Get All Estimators

```{r}
tribble(
  ~estimator, ~estimand, ~sample,
  "SATE", "ATE", T,
  "PATE", "ATE", F,
  "SATT", "ATT", T,
  "PATT", "ATT", F,
  "SATC", "ATC", T,
  "PATC", "ATC", F
) %>%
  rowwise() %>%
  mutate(match = list(get_match(estimand, sample))) %>%
  tidyr::unnest_wider(match) %>%
  select(-estimand, -sample) %>%
  kbl(booktabs = T, linesep = "") %>%
  kable_styling(position = "center") %>%
  kable_styling(latex_options = c("striped", "hold_position"))
```

\newpage
# Appendix: Replicating Stata's Robust Standard Errors

Custom function by Jorge Cimentada that is used to replicate the robust standard errors in Stata:^[https://cimentadaj.github.io/blog/2016-09-19-obtaining-robust-standard-errors-and-odds-ratios/obtaining-robust-standard-errors-and-odds-ratios-for-logistic-regression-in-r/]


```{r eval=F}
robustse <- function(x, coef = c("logit", "odd.ratio", "probs")) {
  suppressMessages(suppressWarnings(library(lmtest)))
  suppressMessages(suppressWarnings(library(sandwich)))

  sandwich1 <- function(object, ...) {
    sandwich(object) *
      nobs(object) / (nobs(object) - 1)
  }
  # Function calculates SE's
  mod1 <- coeftest(x, vcov = sandwich1)
  # apply the function over the variance-covariance matrix

  if (coef == "logit") {
    return(mod1) # return logit with robust SE's
  } else if (coef == "odd.ratio") {
    mod1[, 1] <- exp(mod1[, 1]) # return odd ratios with robust SE's
    mod1[, 2] <- mod1[, 1] * mod1[, 2]
    return(mod1)
  } else {
    mod1[, 1] <- (mod1[, 1] / 4) # return probabilites with robust SE's
    mod1[, 2] <- mod1[, 2] / 4
    return(mod1)
  }
}
```